{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads csv file into a pandas dataframe\n",
    "articles = pd.read_csv(Path(\"data\") / \"articles.csv\")\n",
    "\n",
    "# Adds a new columns called node_id which corresponds to the index\n",
    "articles[\"node_id\"] = articles.index\n",
    "\n",
    "# Make List into an array\n",
    "articles[\"lists\"] = articles[\"lists\"].str.split(\"; \")\n",
    "\n",
    "### REMOVE THIS BEFORE SUBMISSION HERE I AM TAKING A SAMPLE\n",
    "#articles = articles.sample(n=10, random_state=42)\n",
    "test_data = pd.read_csv(Path(\"data\") / \"test_data.csv\")\n",
    "train_data = pd.read_csv(Path(\"data\") / \"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_graph = nx.Graph()\n",
    "medium_graph.add_nodes_from(articles[\"node_id\"].to_list())\n",
    "\n",
    "list_to_nodes = defaultdict(set)\n",
    "for _, row in articles[[\"node_id\", \"lists\"]].iterrows():\n",
    "    for l in row[\"lists\"]:\n",
    "        list_to_nodes[l].add(row[\"node_id\"])\n",
    "\n",
    "for node_ids in list_to_nodes.values():\n",
    "    medium_graph.add_edges_from(combinations(node_ids, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make the walks now to get the node embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Random Walk\n",
    "def random_walks(graph: nx.Graph, num_walks: int, walk_length: int) -> np.ndarray:\n",
    "    result = []\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        for i in range(num_walks):\n",
    "            walk = [node]\n",
    "            for j in range(walk_length - 1):\n",
    "                current_node = walk[-1]\n",
    "                neighbors_list = list(graph.neighbors(current_node))\n",
    "                \n",
    "                if len(neighbors_list) == 0:\n",
    "                    walk.append(node)\n",
    "                    continue        \n",
    "\n",
    "                # Randomly choose a neighbor\n",
    "                index = np.random.randint(len(neighbors_list))\n",
    "                next_node = neighbors_list[index]\n",
    "                walk.append(next_node)\n",
    "                \n",
    "             # Pad shorter walks with None as we want to create equal length np arrays   \n",
    "            if len(walk) < walk_length:\n",
    "                walk.extend([node] * (walk_length - len(walk)))\n",
    "            result.append(walk)\n",
    "\n",
    "    result = [[str(w) for w in walk] for walk in result]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 27718 nodes and 2014162 edges\n"
     ]
    }
   ],
   "source": [
    "print(medium_graph)\n",
    "\n",
    "depth = 100\n",
    "walks = random_walks(medium_graph, num_walks= 5, walk_length = depth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use the word2vec model to embed these walks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take the context vector to be the mode at each index in the walk.\n",
    "\n",
    "model = Word2Vec(\n",
    "    walks,\n",
    "    vector_size=128,  # Dimensionality of the embeddings\n",
    "    window=5,         # Context window size\n",
    "    min_count=0,      # Ignore words with frequency below this\n",
    "    sg=1,             # Use skip-gram model\n",
    "    workers=4,        # Number of threads to use\n",
    "    epochs=10         # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_model.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphS = nx.relabel_nodes(medium_graph, lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeEmbeddings = {}\n",
    "\n",
    "for node in medium_graph.nodes():\n",
    "\n",
    "    v = model.wv[str(node)]\n",
    "    nodeEmbeddings[node] = v\n",
    "    # print(v.shape)\n",
    "\n",
    "# 128 dim vector embedding for each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Classifier training in the embedding space. \n",
    "\n",
    "Now that we have the embedding, we can trian a kNN classifier given the training data. \n",
    "We will do nearest neighbours in the embedding space and use euclidean distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nodeEmbeddings.keys())\n",
    "\n",
    "# We can train on the train data\n",
    "\n",
    "trainEmbeddings = []\n",
    "trainLabels = train_data[\"label\"].to_list()\n",
    "\n",
    "for node_id in train_data[\"node_id\"]:\n",
    "    trainEmbeddings.append(nodeEmbeddings[node_id])\n",
    "\n",
    "trainEmbeddings = np.array(trainEmbeddings)\n",
    "trainLabels = np.array(trainLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "def distance(p1,p2):\n",
    "    return np.linalg.norm(p1-p2)\n",
    "\n",
    "\n",
    "def getMajority(labels):\n",
    "    # Count occurrences of each label\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    \n",
    "    # Sort labels by count in descending order\n",
    "    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the n most common labels\n",
    "    return [label for label, _ in sorted_labels[:k]]\n",
    "\n",
    "\n",
    "def kNN(nod_id):\n",
    "    # We find the nearest k neighbours in embedding space\n",
    "    majorityLabel = None\n",
    "\n",
    "    v = np.array(nodeEmbeddings[nod_id])\n",
    "\n",
    "    distances = np.array([distance(v,e1) for e1 in trainEmbeddings])\n",
    "\n",
    "    firstK = np.argsort(distances)[:k]\n",
    "\n",
    "    closestKLabels = trainLabels[firstK]\n",
    "\n",
    "    majorityLabel = getMajority(closestKLabels)[0]\n",
    "\n",
    "    return majorityLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use the kNN on the test data\n",
    "\n",
    "testEmbeddings = []\n",
    "testActualLabels = test_data[\"label\"].to_list()\n",
    "\n",
    "for node_id in test_data[\"node_id\"]:\n",
    "    testEmbeddings.append(nodeEmbeddings[node_id])\n",
    "\n",
    "testEmbeddings = np.array(testEmbeddings)\n",
    "\n",
    "\n",
    "\n",
    "totalCount = 0\n",
    "correct = 0\n",
    "\n",
    "i = 0\n",
    "for node_id in test_data[\"node_id\"]:\n",
    "    prediction = kNN(test_data[\"node_id\"][i])\n",
    "    actual = testActualLabels[i]\n",
    "    totalCount += 1\n",
    "    if prediction == actual:\n",
    "        correct += 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n",
    "accuracy = correct/totalCount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8007594936708861\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
